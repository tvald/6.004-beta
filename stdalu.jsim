* CNXN *
.subckt cnxn a b
.connect a b
.ends


* INCREMENTER *
.subckt incrementer a ci s co
Xs  a ci s  xor2
Xco a ci co and2
.ends


* FULL ADDER *
.subckt fulladder a b ci s co
Xxor1 a b  1 xor2
Xxor2 1 ci s xor2
Xnand1 a  b  ab nand2
Xnand2 b  ci bc nand2
Xnand3 ci a  ca nand2
Xnand4 ab bc ca co nand3
.ends


* x32 RIPPLE ADDER *
.subckt adder32 ALUFN[0] A[31:0] B[31:0] s[31:0] z v n
Xxb B[31:0] ALUFN[0]#32 XB[31:0] xor2
Xfa A[31:0] XB[31:0] C[30:0] ALUFN[0] s[31:0] C[31:0] fulladder

Xznor s[31:0] znor[7:0] nor4
Xznan znor[7:0] znan[1:0] nand4
Xzout znan[1:0] z nor2

Xvn s[31] A[31] XB[31] _s31 _A31 _XB31 inverter
Xv0 A[31] XB[31] _s31  vx1 nand3
Xv1 _A31  _XB31  s[31] vx2 nand3
Xv2 vx1 vx2 v nand2

.connect s[31] n
.ends


* x32 CARRY_SELECT ADDER *
.subckt cs_adder8 a[7:0] b[7:0] ci s[7:0] co
Xfa a[7:0] b[7:0] c[6:0] ci s[7:0] co c[6:0] fulladder
.ends

.subckt cs_adder16 a[15:0] b[15:0] ci s[15:0] co
Xfa_low a[7:0]  b[7:0]  ci  s[7:0]      col  cs_adder8
Xfa_hi0 a[15:8] b[15:8] 0   s_hi0[15:8] co0 cs_adder8
Xfa_hi1 a[15:8] b[15:8] vdd s_hi1[15:8] co1 cs_adder8
Xmx_hi  col#9 s_hi0[15:8] co0 s_hi1[15:8] co1 s[15:8] co mux2
.ends

.subckt cs_adder32 ALUFN[0] A[31:0] B[31:0] s[31:0] z v n
Xxb B[31:0] ALUFN[0]#32 XB[31:0] xor2

Xfa_low A[15:0]  XB[15:0]  ALUFN[0] s[15:0]      col cs_adder16
Xfa_hi0 A[31:16] XB[31:16] 0        s_hi0[31:16] co0 cs_adder16
Xfa_hi1 A[31:16] XB[31:16] vdd      s_hi1[31:16] co1 cs_adder16
Xfa_him col#17 s_hi0[31:16] co0 s_hi1[31:16] co1 s[31:16] co mux2

Xznor s[31:0] znor[7:0] nor4
Xznan znor[7:0] znan[1:0] nand4
Xzout znan[1:0] z nor2

Xvn s[31] A[31] XB[31] _s31 _A31 _XB31 inverter
Xv0 A[31] XB[31] _s31  vx1 nand3
Xv1 _A31  _XB31  s[31] vx2 nand3
Xv2 vx1 vx2 v nand2

.connect s[31] n
.ends


* x32 COMPARE *
.subckt compare32 ALUFN[2:1] z v n cmp[31:0]
.connect cmp[31:1] 0
Xm ALUFN[2:1] 0 lt eq lte cmp[0] mux4

.connect eq z
* SIMPLE (Tpd=580ps, 106mc2)
*Xlt n v lt xor2
*Xlte z lt lte or2

* BETTER (Tpd=480ps, 123mc2)
Xltn n v ltn xnor2
Xlt ltn lt inverter
Xzn z zn inverter
Xlte zn ltn lte nand2
.ends


* x32 BOOLEAN *
* Tpd=254ps, 2112mc2
.subckt boole32 ALUFN[3:0] A[31:0] B[31:0] boole[31:0]
Xmux A[31:0] B[31:0] ALUFN[0]#32 ALUFN[1]#32 ALUFN[2]#32 ALUFN[3]#32 boole[31:0] mux4
.ends


* x32 SHIFTER *
.subckt shift32 ALUFN[1:0] A[31:0] B[4:0] shift[31:0]
Xsra ALUFN[1] 0 A[31] sra mux2

* SIMPLE (Tpd=1.802ns, 9531mc2; t=9.5us) *
*Xsl
*+ B[4]#32       B[3]#32       B[2]#32       B[1]#32       B[0]#32
*+ A[31:0]       WL[31:0]      XL[31:0]      YL[31:0]      ZL[31:0]
*+ A[15:0] 0#16  WL[23:0] 0#8  XL[27:0] 0#4  YL[29:0] 0#2  ZL[30:0] 0
*+ WL[31:0]      XL[31:0]      YL[31:0]      ZL[31:0]      SL[31:0]
*+ mux2
*Xsr
*+ B[4]#32          B[3]#32         B[2]#32         B[1]#32         B[0]#32
*+ A[31:0]          WR[31:0]        XR[31:0]        YR[31:0]        ZR[31:0]
*+ sra#16 A[31:16]  sra#8 WR[31:8]  sra#4 XR[31:4]  sra#2 YR[31:2]  sra ZR[31:1]
*+ WR[31:0]         XR[31:0]        YR[31:0]        ZR[31:0]        SR[31:0]
*+ mux2
*Xs ALUFN[0]#32 SL[31:0] SR[31:0] shift[31:0] mux2

* BETTER (Tpd=1.828ns, 6075mc2; t=9.5us)
Xsl
+ ALUFN[0]#32  B[4]#32         B[3]#32        B[2]#32        B[1]#32        B[0]#32      ALUFN[0]#32
+ A[31:0]      V[31:0]         W[31:0]        X[31:0]        Y[31:0]        Z[31:0]      Q[31:0]
+ A[0:31]      V[15:0] sra#16  W[23:0] sra#8  X[27:0] sra#4  Y[29:0] sra#2  Z[30:0] sra  Q[0:31]
+ V[31:0]      W[31:0]         X[31:0]        Y[31:0]        Z[31:0]        Q[31:0]      shift[31:0]
+ mux2

.ends


.subckt alu ALUFN[5:0] _A[31:0] _B[31:0] alu[31:0]

Xbuf _A[31:0] _B[31:0] A[31:0] B[31:0] buffer_2

//Xadd ALUFN[0] A[31:0] B[31:0] s[31:0] z v n adder32
Xadd ALUFN[0] A[31:0] B[31:0] s[31:0] z v n cs_adder32
Xcmp ALUFN[2:1] z v n cmp[31:0] compare32
Xboo ALUFN[3:0] A[31:0] B[31:0] boole[31:0] boole32
Xshf ALUFN[1:0] A[31:0] B[4:0] shift[31:0] shift32

Xalu ALUFN[4]#32 ALUFN[5]#32 s[31:0] boole[31:0] shift[31:0] cmp[31:0] alu[31:0] mux4

.ends







